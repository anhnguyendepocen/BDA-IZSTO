---
title: "Bayesian Data Analysis for Medical Data"
subtitle: Simple Linear Regression
author      : Paola Berchialla
job         : Department of Clinical and Biological Sciences, University of Torino
output:
  ioslides_presentation:
    css: assets/css/ioslides.css
    logo: assets/img/dscb.jpg
    mathjax: local
    self_contained: no
    widescreen: yes
  beamer_presentation: default
  slidy_presentation: default
---

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
 </style>


## Why linear regression

- We recognize a set of measurements and we want to make predictions or quantify relationships between _outcome_ variable and _predictors_
- For the _outcome variable_ we define a __likelihood__ distribution that defines 
the plausability of individual observations
    - in linear regressione, the distribution is always Gaussian
- We relate the the mean of likelihood distribution to a linear  


## A simple motivating example: Plasma dataset

```{r, }
load('data\\plasma.RData')

```

- Observational studies have suggested that low dietary intake or low plasma concentrations of retinol, beta-carotene, or other carotenoids might be associated with increased risk of developing certain types of cancer. 

- A cross-sectional study has been designed to investigate the relationship between personal characteristics and dietary factors, plasma concentrations of retinol, beta-carotene and other carotenoids. 

- Study subjects (N = 315) were patients who had an elective surgical procedure during a three-year period to biopsy or remove a lesion of the lung, colon, breast, skin, ovary or uterus that was found to be non-cancerous. We display the data for only two of the analytes.

## A simple motivating example: Plasma dataset

```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.align='center'}
load('data\\plasma.RData')

library(ggplot2)
with(plasma, 
  qplot(log(betaplasma), betadiet)+geom_smooth(method=lm, se=F))
```

## A simple frequentist model{.smaller}

```{r, message=FALSE, warning=FALSE}
library(arm)
mod_f <- lm(log(betaplasma) ~ betadiet, data = plasma, subset=betaplasma>0)

arm::display(mod_f)
confint(mod_f)
```


## A simple Bayesian model 

$$
Y_i \sim \textrm{Normal}(\beta\times X_i, \sigma) 
$$



```{r}
mod_f <- lm(log(betaplasma) ~ betadiet, data = plasma, subset=betaplasma>0)
simple_bayesian_lm <- sim(mod_f, n.sims = 1000)
cat('Credible intervals for the model parameters:\n')
apply(coef(simple_bayesian_lm), 2, quantile, prob = c(0.025, 0.975))

```

## A simple Bayesian model 
```{r}
cat('\nCredible interval for the estimated residual standard deviation:\n')
quantile(simple_bayesian_lm@sigma, prob = c(0.025, 0.975))
```


## A simple Bayesian model 

- Bayesian methods get a posterior probability for specific hypotheses:
      - the slope parameter is greater than 0? 
      - the slope parameter is greater than 0.001?

```{r, echo=FALSE}
cat('Probability the slope parameter is > 0: ', sum(coef(simple_bayesian_lm)[,2]>0)/1000)

cat('Probability the slope parameter is > 0.0001: ',
sum(coef(simple_bayesian_lm)[,2]>.0001)/1000)

```

## Plot the effect of X on Y


```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
require(gridExtra)

plot1 <- with(plasma, 
  qplot(betadiet, log(betaplasma))) + 
  geom_abline(intercept = coef(simple_bayesian_lm)[,1], slope = coef(simple_bayesian_lm)[,2],
              col=rgb(0,0,0,0.05))
    

newdat <- data.frame(x=seq(0, 10000, by = 100))
newmodmat <- model.matrix(~x, data=newdat)
fitmat <-  matrix(ncol=1000, nrow=nrow(newdat))
for(i in 1:1000) fitmat[,i] <- newmodmat %*% coef(simple_bayesian_lm)[i,]


plot2 <- with(plasma, 
  qplot(betadiet, log(betaplasma))) + 
  geom_smooth(aes(x = newdat$x, y = apply(fitmat, 1, quantile, probs = 0.025)), size = 0.5,
              color = 'black', linetype = 'dotted')+
   geom_smooth(aes(x = newdat$x, y = apply(fitmat, 1, quantile, probs = 0.5)), size = 1,
              color = 'black', linetype = 'solid')+
   geom_smooth(aes(x = newdat$x, y = apply(fitmat, 1, quantile, probs = 0.975)), size = 0.5,
              color = 'black', linetype = 'dotted')
  

grid.arrange(plot1, plot2, ncol=2)
```

## Predictive distribution

- We can get a sample of random draws from the posterior predictive distribution
$$
y^*\vert\beta,\sigma^2,X \sim \textrm{Normal}(X\beta, \sigma^2)
$$
using the simulated joint posterior distributions of the model parameters
    - it allows for taking the uncertainty of the parameter estimates
- A new value $y^*$ is drawn from the posterior distribution $\textrm{Normal}(X\beta, \sigma^2)$ for each simulated set of model parameters
- 2.5% and 97.5% quantiles of the predictive distribution can be visualize for each x value

## Predictive distribution

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.align='center'}
newdat <- data.frame(x=seq(0, 10000, by = 100))
newmodmat <- model.matrix(~x, data=newdat)
fitmat <-  matrix(ncol=1000, nrow=nrow(newdat))
for(i in 1:1000) fitmat[,i] <- newmodmat %*% coef(simple_bayesian_lm)[i,]

newy <-  matrix(ncol=1000, nrow = nrow(newdat))

for(i in 1:1000) newy[,i] <-  rnorm(nrow(newdat), mean = fitmat[,i],
sd=simple_bayesian_lm@sigma[i])

plot2 +
  geom_smooth(aes(x = newdat$x, y = apply(newy, 1, quantile, probs = 0.025)), size = 0.5,
              color = 'black', linetype = 'dashed')+
  geom_smooth(aes(x = newdat$x, y = apply(newy, 1, quantile, probs = 0.975)), size = 0.5,
              color = 'black', linetype = 'dashed')
  
```

## Predictive distribution
- Future observations are expected to be within the interval defined by the dashed lines in  with a probability of 95% 
- Increasing sample size will not give a narrower, but a more precise predictive distribution
- once we have a simulated sample of the posterior
predictive distribution, we can give an estimate for
the proportion of observations greater than any relevant thresholds

```{r, echo=FALSE}
cat('Probability a future observation with x = 5000 is higher than 5:\n')
```

```{r}
sum(newy[newdat$x==5000,]>5)/1000
```



## What does sim do?

- In the Bayesian framework we are interested in the joint posterior distribution of $\mathbf{\beta} = (\beta_0, \beta_1)$ and the residual variance $\sigma^2$

$$
P(\mathbf{\beta}, \sigma^2 \vert \mathbf{y, X}) = P(\mathbf{\beta}\vert \sigma^2 \mathbf{y, X}) \times P( \sigma^2 \vert \mathbf{y, X})
$$
- The conditional posterior distribution  $P(\mathbf{\beta}\vert, \sigma^2 \mathbf{y, X})$ of $\mathbf\beta$ is the posterior distribution of $\mathbf\beta$ given a specific value of $\sigma^2$

- __sim__ simulates 1000 values from the joint posterior distribution of the three model parameters:
    - draws a random value from the marginal posterior distribution $\sigma^2$ 
    - draws random values from the conditional posterior distribution for $\mathbf\beta$

## What does sim do?

$$
P(\mathbf{\beta}, \sigma^2 \vert \mathbf{y, X}) = P(\mathbf{\beta}\vert \sigma^2 \mathbf{y, X}) \times P( \sigma^2 \vert \mathbf{y, X})
$$

- $P(\mathbf{\beta}\vert \sigma^2 \mathbf{y, X})$ can be analytically derived
    - with __flat prior distributions__ it is uni(or multi)--variate _Normal_ distribution $\textrm{Normal}(\mathbf\beta), V_\beta\sigma^2)$ 
    - For models with the normal error distribution, estimates for $\mathbf\beta$ equal ML estimates
    

## What does sim do?

- The marginal posterior distribution of $\sigma^2$ is independent of specific values of $\mathbf\beta$ 
    - with __flat prior distributions__ it is an _inverse_ $\chi^2$ distribution $P( \sigma^2 \vert \mathbf{y, X}) = \textrm{Inv-}\chi^2(n-k,s )$
    
- The marginal posterior distribution of $\mathbf\beta$ can be obtained by integrating the conditional posterior distribution $P(\mathbf{\beta}\vert \sigma^2 \mathbf{y, X}) = \textrm{Normal}(\mathbf\beta), V_\beta\sigma^2)$ over the distribution of $\sigma^2$
    - it results in a uni-multivariate $t$-distribution.

## What does sim do?

_sim_ uses __improper priors__:
$$
\begin{align}
p(\beta) & \propto 1\\
p(\sigma^2) &\propto 1/\sigma^2 \\
\end{align}
$$
- These priors are called __improper__ because they are not proper probability distribution since their density function do not integrate to 1

- $p(\beta)$ is a uniform distribution (a horizontal line at 1)
- $p(\sigma^2) \propto 1/\sigma^2$ is equivalent to uniform distribution on $\log\sigma$


## Describing the model

<center>
<img src="images/HierarchicalDiagram.jpg" />
</center>    


## A language for describing the model
$$
\begin{align}
\textrm{outcome}_i &\sim \textrm{Normal}(\mu_i, \sigma)\\
\mu_i & = \beta\times\textrm{predictor}_i\\
\beta &\sim \textrm{Normal}(0, 10)\\
\sigma & \sim \textrm{HalfCauchy}(0,1)
\end{align}
$$
- $\beta$ and $\sigma$ have now proper, __weakly informative__ priors

## Stan: specify the data

Do you remember?
- _Important annoying fact #1: STAN needs data as a list not a dataframe_
- specify data as well as meta data (i.e. the number of groups)


```{r}
plasma_g0 <- subset(plasma, betaplasma>0)
plasma_dat <- list(N = 314 , #specify number of observations as a scalar
                    log_betaplasma = log(plasma_g0$betaplasma), # data vector
                    betadiet = plasma_g0$betadiet # data vector (predictor) 
                    )
```

## Stan: write your code

```{r}

model_string <- 'data {
  // First we declare all of our variables in the data block
  int<lower=0> N;// Number of observations
  vector[N] log_betaplasma; //Specify the outcome as a vector
  vector[N] betadiet;  //Specify the covariate as a vector
}
parameters {
  vector[2] beta; // Betas are a vector of length 2 (intercept and slope)
  real<lower=0> sigma; //error parameter
}
model {
  //Priors
  beta[1] ~ normal(0, 10); //intercept
  beta[2] ~ normal(0, 5); //slope
  sigma ~ cauchy(0, 5); //error
  log_betaplasma ~ normal(beta[1] + beta[2] * betadiet, sigma);
}'

```


## Compile the model

```{r dso, message=FALSE, warning=FALSE, cache=TRUE, eval=FALSE}
library(rstan)
stanDso <- stan_model( model_code = model_string )
```

## Sample from the posterior distribution {.smaller}

```{r, echo=FALSE}
suppressMessages(library(rstan))
load('output\\lm1_stan.RData')
```


```{r stanfit, cache=TRUE}

stanFit <- sampling(object = stanDso, 
                    data = plasma_dat, 
                    chains = 3, iter = 9000, warmup = 1000, thin = 1)

```

## Print the fit {.smaller}

```{r, results='markup'}
print(stanFit)
```

- __n_eff__: effective sample size, a measure of autocorrelation among samples 
- __Rhat__: split-chain convergence diagnostic 
    - >1.1 suggests poor convergance

## Posterior distributions


```{r sampling, dependson='stanfit', message=FALSE}
samples <- extract(stanFit)
beta_post <- samples[['beta']]
```

```{r, echo=FALSE, fig.width=4, message=FALSE, fig.align='center'}
plot1 <- qplot(beta_post[,1]) # intercept posterior samples
plot2 <- qplot(beta_post[,2]) # slope posterior samples

grid.arrange(plot1, plot2, ncol=2)
```

## Plot credible intervals{.smaller}

```{r, dependson=c('sampling','stanfit'), eval=FALSE}
plot(stanFit, pars=c('beta', 'sigma'))
```


## Plot credible intervals{.smaller}

```{r, dependson=c('sampling','stanfit'), echo=FALSE, results='asis', fig.align='center', fig.heigth=3}
plot(stanFit, pars=c('beta', 'sigma'))
```


## Check the fit with MLE estimates

```{r, dependson=c('sampling','stanfit'), echo=FALSE, fig.align='center'}
int <- median(beta_post[,1]) # posterior intercept estimate
slope <- median(beta_post[,2])# posterior slope estimate

qplot(plasma_g0$betadiet,log(plasma_g0$betaplasma))+
  geom_smooth(method=lm,se=F)+
  geom_abline(intercept=int,slope=slope,color='red')

```


## View trace plots of each parameter (fuzzier is better)
```{r, dependson=c('sampling','stanfit')}
traceplot(stanFit, ncol = 1)
```


## Getting the slides

* The slides for this course were created with Rmarkdown: [http://rmarkdown.rstudio.com/](http://rmarkdown.rstudio.com/). 
* They are available from [https://github.com/berkeley3/BDA-IZSTO](https://github.com/berkeley3/BDA-IZSTO).
* To re-compile the slides:

    + Download the directory containing the lectures from Github
    + In R open the .Rmd file and set the working directory to the lecture directory
    + Click the *Knit* button on Rstudio or run the following commands: 
  
```{r RmarkdownChunk, eval=FALSE}
library(rmarkdown) 
render("main.Rmd")
```
